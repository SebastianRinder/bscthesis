\chapter{Foundations}
\label{chap:2}

- Ei use max mean

- hyper parameter Optimization
-- added bound stuff

\section{Global Bayesian optimization}

To find the maximum of our expensive black box function we use Bayesian optimization. It makes the search process more efficient by incorporating a model of the unknown function. This model is used to guide the exploration for new points.

\begin{algorithm}
    \caption{Global Bayesian optimization\label{alg:boGlob}}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{$X$: $m$ uniformly random samples from the search space\\$s$: number of BO iterations}
    \Output{$x_{s+m}$}
    \BlankLine

    $y$ = evaluations of the objective at the points $X$\\
    \For{n = $m$ \KwTo $s$ + $m$}{
        compute $K(X,X)$\\
        $x_{n+1}$ = point at the optimum of the acquisition function\\
        $y_{n+1}$ = evaluation of the objective at the point $x_{n+1}$\\
        $X = \{X, x_{n+1}\}$\\
        $y = \{y, y_{n+1}\}$\\
    }
\end{algorithm}

During the acquisition function optimization, $K(X,X)$ from the Gaussian process does not change, so it is precomputed. The values for the initial sample count $m$ and the iteration count $s$ are typically set to about 10 and a few hundred respectively.

\section{Acquisition function}
The core of the Bayesian optimization consists of selecting the next evaluation point in our search space. We get that point by optimizing a so called acquisition function. It depends on the incumbent Gaussian process model of the true objective. We choose expected improvement during the global bayesian optimization and in the local optimization (chapter \ref{chap:contributions}) we use Thompson sampling. Both acquisition functions take the mean und the variance from the gaussian process model into account to guide the exploration process. The difficulty lies in avoiding excessive exploration or exploitation. Exploration looks for points with a high variance and exploitation selects points with a high mean instead. The latter one would result in a local optimum whereas too much exploration may not improve at all.

\subsection{Expected improvement}
To get an expected improvement function value at a test point $x_*$ we need the mean value $\mu(x_*)$, and the standard deviation value $\sigma(x_*) = \sqrt{v(x_*)}$ from the Gaussian process model. Also we need the maximum of all observations $y_{max}$ and a trade-off parameter $\tau$. For the cumulative distribution function and the probability density function from the Gaussian distribution we write $\Phi(.)$ and $\phi(.)$ respectively. They both have zero mean and unit variance.
We adopt the expected improvement function
\[
    \mathrm{EI}(x_*)=
\begin{cases}
    (\mu(x_*) - y_{max} - \tau)\Phi(\mathrm{z}(x_*))+\sigma(x_*)\phi(\mathrm{z}(x_*))& \text{if } \sigma(x_*)> 0\\
    0 & \text{if } \sigma(x_*)= 0
\end{cases}
\]
where
\[
    \mathrm{z}(x_*)=
\begin{cases}
    \frac{(\mu(x_*) -y_{max} - \tau)}{\sigma(x_*)} & \text{if } \sigma(x_*)> 0\\
    0 & \text{if } \sigma(x)= 0
\end{cases}
\]
as suggested in \cite{brochu2010tutorial}. The trade-off parameter $\tau$ is set to 0.01 accordingly. The expected improvement function is then optimized over the whole search space to give us the next evaluation point

$$x_{n+1} = \arg \max_{x_*} \mathrm{EI(x_*)}.$$

\subsection{Thompson sampling}
For the Thompson sampling we sample one function from the Gaussian process posterior,

$$\mathrm{TS}\sim \mathrm{GP}(0,K(X,X_*)),$$

where $X$ is the dataset of already evaluated points, and $X_*$ is a randomly Gaussian distributed set of points with mean and variance given by the local optimzer. These mean and variance represent our current search space.
To draw function values we need the mean vector $\mu$ and the full covariance matrix $V$ from the Gaussian process model. First we take the lower Cholesky decomposite of $V$ such that $L_V L_V^\top = V$. Then we compute a vector $g$, which consists of independent Gaussian distributed valus with zero mean and unit variance. Finally we get a vector $\mathrm{TS}$ of sampled values:

$$\mathrm{TS}(x_*) = \mu + L_V g.$$

We take the one with the highest value such that,

$$x_{n+1} = \arg \max_{x_*} \mathrm{TS}(x_*),$$

to get the next evaluation point.

\section{Gaussian Process Regression}
Since we want to estimate an objective function in a machine learning environment we elect to use Gaussian process regression. It fits a multivariate gaussian distribution over our prior data. From the regression we get a posterior mean and variance which describe our model of the objective function. The mean represents a prediction of the true objective at a given point and the variance represents the uncertainty at that point. The more points our model incorporates the smaller the variance, and the preciser the predictions, in the proximity around prior points.\\
In real world applications we always have some noise in our objective observations. Therefore a Gaussian distributed error term,

$$\epsilon \sim \mathcal{N}(0,\sigma_n^2),$$

with zero mean and $\sigma_n^2$ variance is added to the function value. The observed target

$$y_{n} = f(x) + \epsilon$$

regards this noise. Before doing regression we transform our observations to zero mean and uniform variance:

$$y = \frac{y_{n}-\mathrm{mean}(y_{n})}{\mathrm{std}(y_{n})}.$$

The knowledge our training data provides is represented by the kernel matrix $K(X,X)$. With a matrix of test points $X_*$ we get the joint distribution of the normalized target values and the function values at the test locations:

$$\left[ \begin{array}{c} y \\ f_* \end{array} \right] \sim \mathcal{N} \left(0, \begin{bmatrix} K(X,X)+\sigma_n^2 I & K(X,X_*) \\ K(X_*,X) & K(X_*,X_*) \end{bmatrix} \right)$$

Now we can calculate the posterior mean and variance at given test points $X_*$.

\begin{align}
    K_n &= K(X,X)+\sigma_n^2 I \label{eq:kNoise} \\
    \mu &= K(X_*,X)K_n(X,X)^{-1}y \label{eq:meanGauss} \\
    V &= K(X_*,X_*)-K(X_*,X)K_n(X,X)^{-1}K(X_*,X)^\top \label{eq:wholeVar} \\
    \sigma^2 = v &= \mathrm{diag}(K(X_*,X_*)-K(X_*,X)K_n(X,X)^{-1}K(X_*,X)^\top) \label{eq:vectorVar} \\ \nonumber
\end{align}

The vectors $\mu$ and $v$ hold the means and variances for all test points. Also we get the whole covariance matrix $V$.

\section{Markov decision process}
In our reinforcement learning problem we use a Markov decision process model to describe possible decisions as probabilites. Our model consists of a tuple $(S,A,P,P_{0},R)$, holding all states $s \in S$, all actions $a \in A$, all state transitioning probabilities, and all corresponding rewards. Assume an agent which executes a policy $x$ for $T$ time steps receiving a final reward $\bar{R}(\xi)$. This reward depending on a policy is what we want to maximize.\\
We formulate the conditional probability of observing trajectory $\xi$ given policy $x$ by

$$P(\xi|x) = P_{0}(s_{0}) \prod_{t=1}^{T} P(s_{t}|s_{t-1}, a_{t-1}) P_{\pi}(a_{t-1}|s_{t-1}, x)$$

in which trajectory $\xi=(s_{0},a_{0}, ..., s_{T-1}, a_{T-1}, s_{T})$ contains the sequence of state, action tuples and $x \in \mathbb{R}^{D}$ a set of $d$ policy parameters. $P_{0}(s_{0})$ is the probability of starting in the initial state $s_{0}$. $P(s_{t}|s_{t-1}, a_{t-1})$ is the probability of transitioning from state $s_{t-1}$ to $s_{t}$ when action $a_{t-1}$ is executed. The stochastic mapping $P_{\pi}(a_{t-1}|s_{t-1},x)$ is the probability for selecting the action $a_{t-1}$ when in state $s_{t-1}$ and executing the parametric policy $x$. So we receive a final reward for a sampled trajectory,

$$\bar{R}(\xi)=\sum_{t=1}^T R(s_{t-1}, a_{t-1}, s_{t}),$$

which is the sum of all immediate rewards, given by an rewarding function $R(s_{t-1}, a_{t-1}, s_{t})$. This rewarding function depends on the given environment. For example it rewards a state we want to achieve by returning a value greater than zero and can penalize states we do not want our agent to be in by returning negative values.

\section{Kernel for Gaussian process}
\subsection{Squared exponential kernel}
$$D(x_i,x_j) = x_i-x_j$$
$$K(x_i,x_j,\sigma) = \sigma_{f}^{2}\,\exp\left(-\frac{D^2(x_i,x_j)}{2\sigma_l^2} \right) $$
\subsection{Matern 5/2 kernel}
$$D(x_i,x_j) = x_i-x_j$$
$$K(x_i,x_j,\sigma) = \sigma_{f}^{2}\,\exp\left(-\frac{D^2(x_i,x_j)}{2\sigma_l^2} \right) $$
\subsection{Trajectory kernel}
Standard kernels like the squared exponential kernel, relate policies by measuring the difference between policy parameter values. Therefore policies with similar behavior but different parameters are not compared adequately. The behavior based Trajectory kernel fixes this, by relating policies to their resulting behavior. This makes our policy search more efficient, since we avoid redundant search of different policies with similar behaviour. To examine the difference between two policies $x_{ i }$ and $x_{ j }$ the discrete Kullback Leibler divergence

$$D_{\mathrm {KL} }(P(\xi|x_{ i })||P(\xi|x_{ j })) = \sum _{i}P(\xi|x_{ i })\,\log {\frac {P(\xi|x_{ i })}{P(\xi|x_{ j })}}.$$

is applied to the respective policy-trajectory mappings $P(\xi|x_{ i })$ and $P(\xi|x_{ j })$. It measures how the two probability distributions diverge from another.\\

In general $D_{\mathrm {KL} }(P(\xi|x_{ i })||P(\xi|x_{ j }))$ is not equal to $D_{\mathrm {KL} }(P(\xi|x_{ j })||P(\xi|x_{ i }))$. But we need a symmetric distance measure. So we sum up the two divergences

$$D(x_{ i }, x_{ j }) = D_{\mathrm {KL} }(P(\xi|x_{ i })||P(\xi|x_{ j })) + D_{\mathrm {KL} }(P(\xi|x_{ j })||P(\xi|x_{ i })),$$

to achieve that $D(x_{ i }, x_{ j }) = D(x_{ j }, x_{ i })$. An additional requirement for the kernel is the resulting matrix to be positive semi-definite and scalable\cite{wilson2014using}. Therefore we exponentiate the negative of our distance matrix $D$. We also apply the hyper parameters $\sigma_f$ to compensate for the signal variance and $\sigma_l$ to adjust for signal scale. This gives us the covariance matrix

\begin{equation} \label{eq:trajKernel}
K(x_{ i },x_{ j },\sigma_f,\sigma_l) = \sigma_f \exp(-\sigma_l D(x_{ i }, x_{ j })),
\end{equation}
for the gaussian process.


\section{Hyper parameter optimization}
Selecting proper hyper parameters for the Gaussian process regression enhances the efficiency of our learning algorithm. Also it helps avoiding numerical problems. To find an optimum for the signal variance hyperparameter $\sigma_f$ and the length scale hyperparameter $\sigma_l$ we maximize the log marginal likelihood function
\begin{equation} \label{eq:hypOpt}
    \log p(y|x,\sigma_f,\sigma_l) = -\frac{1}{2} y^\top K_n^{-1} y -\frac{1}{2} \log |K_n| -\frac{n}{2}\log 2\pi ,
\end{equation}

from our Gaussian process. Where $n$ is the number of observations, $x$ is the $d \times n$ dataset of inputs and $K_n$ is the covariance matrix for the noisy target $y$.
