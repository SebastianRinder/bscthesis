\chapter{Introduction}
\label{chap:0}

The science of machine learning aims at creating well-functioning systems that are programmed to learn from data. This enables such systems to continuously improve their performance autonomously in order to accomplish a certain task. Machine learning is particularly important for tasks where manual design shows poor performance, such as image or speech recognition, effective web search or realization of self-driving cars. Considering the huge amount of data to be processed, machine learning algorithms easily surpass deterministic design. The advantage stems from generalizing learned examples. Accordingly, patterns in the learning data can be recognized and transferred to future situations without human supervision.\\
\\
The goal of reinforcement learning, a sub-area of machine learning, is to teach an agent how to improve its behaviour \cite{sutton1998reinforcement}. In order to achieve a certain result, behaviour that contributes to the desired result is reinforced and behaviour that is not needed to achieve this result will be discouraged. The following example illustrates this. As an agent let us assume a robot arm with a table tennis racket and a ball. If our goal is to balance the ball for as long as possible, the agent will be penalized if it drops the ball, and rewarded for each successful time step of balancing. At the end of an evaluation episode all
rewards received are summarized to get the cumulative reward as a measurement of performance.\\
\\
The agent's behaviour can be defined by a set of policy parameters. These parameters tell the agent which action to perform depending on its current state. State parameters of the table tennis agent, for example, would contain the angle of the racket and the position of the ball. A well-tuned policy could tell the robot arm when and to what extent it should rotate in order to prevent the ball from falling. Finding such a well performing policy is the goal of our reinforcement learning task.\\
\\
One approach of maximizing the performance of an agent could try random policies until finding a sufficient one. Unfortunately, this would take a very long time because it is highly unlikely to find suitable policy parameters by accident. Especially in more complex environments with higher dimensional policies. Besides the huge number of evaluations necessary, we would have to deal with wear and the limited movement speed of our robotic equipment.\\
\\
For this reason, we need a more efficient search process to find properly functioning policies. A search process, which can learn from collected data, and extrapolate future behaviour. In recent years the Bayesian optimization approach has been shown to be very efficient in this field \cite{brochu2010tutorial, shahriari2016taking, lizotte2008practical}. It tries to find an optimum of a black box function in as few steps as possible. In our reinforcement learning task the black box function would be the cumulative reward of an episode generated by a specific policy. In this case the optimum would be the policy with the highest cumulative reward achievable.\\
\\
The modelling of the black box function and thus the prediction of future behaviour is based on measuring the distance between already evaluated policies with known results. The outcome of a new policy is then estimated by applying the learned distance metric. Out of many new policies Bayesian optimization selects one with the most promising enhancement. A good enhancement measure will include policies of uncharted areas in the search space and policies with high expected cumulative rewards as well. This balancing act is known as the exploration exploitation trade-off \cite{brochu2010tutorial}. Only exploiting policies with a high expectation value can lead to a local optimum and thus to a result not as good as the best possible one. Whereas excessive exploration will cover the entire search space, but maybe will not find an optimum at all.\\
\\
When using standard kernels, the distance between policies is usually measured by calculating the Euclidean distance between corresponding policy parameters \cite{rasmussen2006gaussian}. A more significant measure would compare the resulting behaviour patterns stemming from certain policies instead of just comparing the parameters of the policies. Such a behaviour measurement is utilised by the trajectory kernel \cite{wilson2014using}, which uses the trajectory data generated by our agent during an evaluation process. This data, consisting of state and action values, relate the respective policies for modelling the black box function. The relation of behavioural patterns has the advantage that different policies with similar results are recognized, and therefore less prioritized by the search. As a consequence, the trajectory kernel can make the search process more efficient, but also has the disadvantage that the effort for kernel computations is greatly increased.\\
\\
When it comes to higher-dimensional problems, Bayesian optimization tends to explore too much because of its focus on global optimization. As a result, the optimum may not be found. Therefore, we use Bayesian optimization with a locally restricted search area \cite{akrour2017local} in addition to the commonly used global Bayesian optimization. The restriction of the search area is adjusted throughout the search process to cover the most promising parts of the search space. Consequently, we only have to optimize locally, which is more robust against high-dimensional problems and also computationally less demanding than global optimization. In order to perform this local optimization proposed by \cite{akrour2017local}, a suitable trajectory kernel is also needed. Therefore we will contribute an additional trajectory kernel to the one we adapted from \cite{wilson2014using}.\\
\\
Accordingly, we will compare the performance of trajectory kernels with standard kernels in both global Bayesian optimization and Bayesian optimization in a local context. The robotic reinforcement learning environments will be given by the classic control tasks of Cart Pole, Mountain Car and Acrobot \cite{sutton1998reinforcement}.\\
