\chapter{Introduction}
\label{chap:0}

In the reinforcement learning area of machine learning we teach an agent how to improve his behaviour. We want a specifc result from that agent, therefore we discourage results we do not seek and reinforce behaviour which leads to our goal. As an agent let us assume a robot arm with a table tennis racket and a ball. If our goal was to balance the ball for as long as possible, we would penalize the agent for letting the ball fall, and reward the agent for each successful time step of balancing. At the end of an evaluation episode we sum up all received rewards to get the cumulative reward as a measure of performance.\\
\\
The agent's behaviour can be defined by a set of policy parameters. These parameters tell the agent how to act depending on his state. A state of the table tennis agent for example would contain the angle of the racket, and the position of the ball. A well tuned policy could tell the arm when to rotate, and at which magnitude, to keep the ball from falling. Finding such a well performing policy is the goal of our reinforcement learning task.\\
\\
One approach of maximizing the performance of an agent could try random policies until finding a sufficient one. Unfortunately this would take a very long time since it is highly unlikely to find fitting policy parameters on accident. Especially in more complex environments the policy can easily have dozens of dimensions. Besides playing the lottery, we would have to deal with wear and tear, and the limited execution speed of our robotic equipment.\\
\\
Therefore we require a more efficient search process for finding properly functioning policies. Optimally a search process, which learns from collected data, and extrapolates future behaviour. In recent years the Bayesian optimization approach has been shown to be very efficient in this field \cite{brochu2010tutorial, shahriari2016taking, lizotte2008practical}. It tries to find an optimum of a black box function in as few steps as possible. In our reinforcement learning task the black box function would be the cumulative reward of an episode generated by a specific policy. And the optimum would be the policy with the highest cumulative reward achievable.\\
\\
The prediction of future behaviour is based on measuring the distance between policies. The outcome of a new policy is then estimated by applying the learned distance metric. Out of many new policies BO selects one with the most promising coverage of the search space. A good coverage will include policies of uncharted areas in the search space and policies with high expected cumulative rewards as well. This balancing act is known as the exploration exploitation trade-off. Only exploiting policies with a high expectation can lead to a local optimum and thus to a result below the best possible one. Whereas excessive exploration will cover the entire search space, but maybe will not find an optimum.\\
\\
When using standard kernels, the distance between policies is usually measured by calculating the Euclidean distance between corresponding policy parameters. A more meaningful measure would compare the resulting behaviours stemming from given policies instead comparing the policies' parameters themselves. We incorporate such a behaviour measure by using the trajectory data generated by our agent. Starting an evaluation episode, the agent is in a specific state. From this state and the given policy an action is sampled. This action determines the next state, from which an action is sampled again. The evaluation process continues until we reach a break condition. This data set generated during an episode is then used by the trajectory kernel. The kernel derives a behaviour measure from the state and action parameter sets, to relate policies accordingly. This relation has the advantage that different policies with similar results are recognized as redundant and are therefore less prioritized by the search. As a consequence the trajectory kernel makes the search process more efficient, but also has the disadvantage that the effort for kernel calculations is greatly increased.\\
\\
When it comes to higher dimensional problems BO tends to over explore because of its focus on global optimization. This can lead to not finding an optimum at all. We therefore also use BO with a locally restriced search area \cite{akrour2017local}. This area is adjusted throughout the search process to cover the most promising parts of the entire search space. As a result, we only have to optimize locally. This method is more robust against high-dimensional problems and also computationally less demanding than global optimization.\\
\\
So we will compare the trajectory kernel to standard kernels in the Bayesian optimization. We also do test runs with global Bayesian optimization and Bayesian optimization in a local context. The reinforcement learning environment will be given by the classic control tasks cart pole, mountain car and acrobot \cite{sutton1998reinforcement}.\\
\\
