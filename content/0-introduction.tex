\chapter{Introduction}
\label{chap:0}

The goal of machine learning science is to create well-functioning systems without programming them explicitly. Especially for tasks where manual design results in poor performance. As in image or speech recognition, effective web search, or realization of self-driving cars. Considering the huge amount of data to be processed, machine learning algorithms easily surpass deterministic design. The advantage stems from learning from examples and then generalizing them. This means that patterns in the learning data can be recognized and transferred to future situations without human supervision.\\
\\
In reinforcement learning, a sub-area of machine learning, we want an agent to teach himself how to improve his behaviour. We want a specifc result from that agent, therefore we discourage results we do not seek and reinforce behaviour which leads to our goal. As an agent let us assume a robot arm with a table tennis racket and a ball. If our goal was to balance the ball for as long as possible, we would penalize the agent for letting the ball fall, and reward the agent for each successful time step of balancing. At the end of an evaluation episode we sum up all received rewards to get the cumulative reward as a measure of performance.\\
\\
The agent's behaviour can be defined by a set of policy parameters. These parameters tell the agent which action to execute depending on the state. A state of the table tennis agent for example would contain the angle of the racket, and the position of the ball. A well tuned policy could tell the arm when to rotate, and at which magnitude, to keep the ball from falling. Finding such a well performing policy is the goal of our reinforcement learning task.\\
\\
One approach of maximizing the performance of an agent could try random policies until finding a sufficient one. Unfortunately this would take a very long time since it is highly unlikely to find fitting policy parameters on accident. Especially in more complex environments with higher dimensional policies. Besides the huge number of evaluations necessary, we would have to deal with wear and tear, and the limited movement speed of our robotic equipment.\\
\\
So we require a more efficient search process for finding properly functioning policies. Optimally a search process, which learns from collected data, and extrapolates future behaviours. In recent years the Bayesian optimization approach has been shown to be very efficient in this field \cite{brochu2010tutorial, shahriari2016taking, lizotte2008practical}. It tries to find an optimum of a black box function in as few steps as possible. In our reinforcement learning task the black box function would be the cumulative reward of an episode generated by a specific policy. And the optimum would be the policy with the highest cumulative reward achievable.\\
\\
The prediction of future behaviour is based on measuring the distance between already evaluated policies with known results. The outcome of a new policy is then estimated by applying the learned distance metric. Out of many new policies Bayesian optimization selects one with the most promising enhancement. A good enhancement measure will include policies of uncharted areas in the search space and policies with high expected cumulative rewards as well. This balancing act is known as the exploration exploitation trade-off. Only exploiting policies with a high expectation can lead to a local optimum and thus to a result below the best possible one. Whereas excessive exploration will cover the entire search space, but maybe will not find an optimum at all.\\
\\
When using standard kernels, the distance between policies is usually measured by calculating the Euclidean distance between corresponding policy parameters. A more meaningful measure would be to compare the resulting behaviors stemming from certain policies rather than comparing the parameters of the policies. We incorporate such a behaviour measure using the trajectory data generated by our agent during an evaluation process. This data, consisting of state and action values, is used by the trajectory kernel to relate the respective policies.\\
\\
