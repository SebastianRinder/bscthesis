\chapter{Experiments}
\label{chap:3}
%

\section{Implementation}

\subsection{Normalization constant}

\subsection{Numerical stability}
in the end -> 1. explain what we want to achieve (no log of small values for example)
2. cholesky
3. hyp param matrix

\subsection{Bayesian Optimizaion algorithm}

\subsection{Cholesky decomposition}
(GP, TS)

\subsection{OpenAI Gym in Python}

\subsection{Gaussian Process Regression}

(difference between full covariance and cov vector)

Instead of calculating the inverse of $K_n$ in \eqref{eq:meanGauss} we use the lower Cholesky decomposed matrix:

$$LL^\top=K_n$$

This is considered faster and numerically more stable \cite{rasmussen2006gaussian}. $K_n^{-1}\,y$ then transforms to:

\begin{equation} \label{eq:regression}
    K_n^{-1}\,y = (L\,L^{T})^{-1}\,y = (L^{-T}\,L^{-1})\,y = L^{-T}\,(L^{-1}\,y) = L^{T}\setminus(L \setminus y).
\end{equation}

The backslash operator denotes the matrix left division, so the solution $x=A\setminus b$ satisfies the system of linear equations $A\,x=b$.
Matrix $K_n$ must be positive definite for the cholesky decomposition. So we double the noice variance hyperparameter $\sigma_n^2$ from \eqref{eq:kNoise} until positive definiteness is achieved.

\subsection{Hyper Parameter Optimization}
Especially for the trajectory kernel we want a hyper parameter optimization, because all the values of the distance matrix $D$ may get very big. When dividing by a well tuned hyper parameter $sigma_l$ before applying the exponential function \eqref{eq:trajKernel}, we avoid getting a zero kernel matrix $K$.\\

When calculating $\log(|K_n|)$ for the hyperparameter optimization \eqref{eq:hypOpt}, again we use the Cholesky decomposition of $K$. Thus the determinant transforms to
$$|K_n|=|L\,L^{T}|=|L|\,|L^{T}|=|L|\,|L|=|L|^{2}.$$

Since the determinant of the Cholesky decomposed matrix,
$$|L| = \textstyle\prod_{i} L_{ii},$$
is the product of its diagonal elements, we can transform this into a numerically more stable version:

$$\log(|K_n|) = \log(|L|^{2}) = 2\,\log(|L|) = 2\,\log(\textstyle\prod_{i} L_{ii}) = 2\textstyle\sum_{i} \log(L_{ii}).$$

The computation of $K_y^{-1}y$ in \eqref{eq:hypOpt} is done by the same method we already use in the Gaussian process \eqref{eq:regression}.

\subsection{Estimation of Trajectory Kernel Function Values}

We estimate the divergence values, because the computational effort will be greatly reduced\cite{wilson2014using}. We use a Monte-Carlo estimate for the approximation

$$\hat{D}(x_{ i }, x_{ j }) = \sum _{\xi \in \xi_i} \log\left( \frac{P(\xi|x_{ i })}{P(\xi|x_{ j })} \right) + \frac{1}{N}\sum _{\xi \in \xi_j} \log\left( \frac{P(\xi|x_{ j })}{P(\xi|x_{ i })} \right) $$

of the divergences between policies with already sampled trajectories. Here $\xi_i$ is the set of trajectories generated by policy $x_i$. For our gaussian process regression we also need a distance measure between a policy with known trajectories and new policies with unknown trajectories. Since there is no closed form solution to this we use the importance sampled divergence

$$\hat{D}(x_{ new }, x_{ j }) = \sum _{\xi \in \xi_j} \left[\frac{P(\xi|x_{ new })}{P(\xi|x_{ j })}\,\log\left(\frac{P(\xi|x_{new})}{P(\xi|x_{j})}\right)+\log\left(\frac{P(\xi|x_{ j })}{P(\xi|x_{ new })}\right)\right] $$

to estimate the divergence between the new policy $x_{new}$ and the policy $x_j$ with already sampled trajectories $\xi_j$.

Since we only have a ratio of transitioning probabilities present in our trajectory kernel we can reduce the logarithmic term to:

\begin{align*}
    \log\left(\frac{P(\xi|x_i)}{P(\xi|x_j)}\right)
    &= \log\left(\frac{P_{0}(s_{0}) \prod_{t=1}^{T} P_s(s_{t}|s_{t-1},a_{t-1}) P_{\pi}(a_{t-1}|s_{t-1},x_i)}{P_{0}(s_{0}) \prod_{t=1}^{T} P_s(s_{t}|s_{t-1},a_{t-1}) P_{\pi}(a_{t-1}|s_{t-1},x_j)}\right)\\
    &= \log\left(\prod_{t=1}^{T}\frac{ P_{\pi}(a_{t-1}|s_{t-1},x_i)}{P_{\pi}(a_{t-1}|s_{t-1},x_j)}\right)\\
    &= \sum_{t=1}^{T} \log \left(\frac{P_{\pi}(a_{t-1}|s_{t-1},x_i)}{P_{\pi}(a_{t-1}|s_{t-1},x_j)}\right)
\end{align*}


Summing up the logarithms in the end is also numerically more stable than taking the logarithm of the whole product.

\subsection{Action selection}

In continuous action space we use a linear policy to action mapping

$$a = f_s(s)^\top x + \epsilon_a,$$

with a small gaussian noise $\epsilon_a$ needed for stochastic policies. So the actions are Gaussian distributed:

$$a \sim \mathcal{N}(f_s(s) x,\epsilon_a^2).$$

Therefore the resulting probability density of the action selection,

$$P_{\pi}(a|s,x) = \frac{1}{\sqrt{2\pi\epsilon_a^2}}\exp\left(-\frac{(a-f_s(s)x)^2}{2\epsilon_a^2}\right),$$

enables us to do the computations of the logarithm of the probability ratios in the trajectory kernel more efficient:

\begin{align*}
    \sum_{t=0}^{T-1} \log \left(\frac{P_{\pi}(a_{t}|s_{t},x_i)}{P_{\pi}(a_{t}|s_{t},x_j)}\right) &= \sum_{t=0}^{T-1} \log \left(\frac{\frac{1}{\sqrt{2\pi\epsilon_a^2}}\exp\left(-\frac{(a_t-f_s(s_t)x_i)^2}{2\epsilon_a^2}\right)}{\frac{1}{\sqrt{2\pi\epsilon_a^2}}\exp\left(-\frac{(a_t-f_s(s_t)x_j)^2}{2\epsilon_a^2}\right)}\right)\\
    &= \sum_{t=0}^{T-1} \log \left( \exp \left( -\frac{(a_t-f_s(s_t)x_i)^2}{2\epsilon_a^2} - \left(-\frac{(a_t-f_s(s_t)x_j)^2}{2\epsilon_a^2}\right)\right)\right)\\
    &= \frac{1}{2\epsilon_a^2} \sum_{t=0}^{T-1} \left((a_t-f_s(s_t)x_j)^2 - (a_t-f_s(s_t)x_i)^2\right).
\end{align*}

The function $f_s(s)$, depending on the state, computes our $d$ dimensional state feature vector.

In discrete action space environments we use a parametric soft-max action selection policy:

$$P(a|s)= \frac{\exp(f_s(s)^\top x_a)}{\sum_{i\in A} \exp(f_s(s)^\top x_i)}.$$

Again it consists of the linear mapping $f_s(s)^\top x$ and $A$ holds all possible actions. The resulting action is then sampled from the probability of action $a$ given state $s$.


\section{Cart pole}
In the cart pole experiment for example it is simply returning the four state values. During the cart pole experiments it has been found useful to threshold the resulting action to $[-1,1]$ to prevent unrealistic behaviour in the simulation.
