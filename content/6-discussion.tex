\chapter{Discussion and Conclusion}
\label{chap:6}

\section{Learning performance}
The continuous Cart Pole MATLAB experiment on global Bayesian optimization (figure \ref{fig:cartpoleMatlabGlobal}) shows a learning performance advantage of our trajectory kernel. It finds well performing policies faster than the other kernels. The Wilson kernel performed far worse than the others. The other kernels have a promising start, but after 20 iterations they stop improving constantly. Instead they find very good and very bad policies, resulting in quite constant mean with a high deviation. This could be an issue of the expected improvement acquisition function, exploring too much after a while. Unfortunately, none of the kernel converges during the 200 Bayesian optimization steps at the maximum possible cumulative reward of 1000. As listed in Table \ref{table:cartpoleMatlabGlobal} a single trial for our trajectory kernel takes almost seven hours on the cluster on average. Therefore we only did 200 iteration steps for the very time consuming runs with the global Bayesian optimization.

The continuous Cart Pole MATLAB experiment on local Bayesian optimization (figure \ref{fig:cartpoleMatlabLocal}) shows a slight performance advantage of our trajectory kernel. Each kernel converges at the maximum after about 400 black-box function evaluations. Presumably due to the well suited Thompson sampling acquisition function in the local optimization context.

The discrete Cart Pole OpenAI Gym experiment on local Bayesian optimization (figure \ref{fig:cartpolePygym}) shows a slight performance disadvantage of our trajectory kernel. In this experiment we have a maximum of 200 time steps per episode, and therefore the highest possible cumulative reward is 200.

The discrete Acrobot OpenAI Gym experiment on local Bayesian optimization (figure \ref{fig:acrobotPygym}) converges at roughly -100. The Acrobot has no definition of solving, but reaching the goal in 100 time steps is fairly good. Our kernel performs worse than the standard kernel in the beginning but catches up towards the end.

The continuous Mountain Car OpenAI Gym experiment on local Bayesian optimization (figure \ref{fig:acrobotPygym}) shows a major performance drawback compared to the standard kernels. But the result is not conclusive because the environment did not solve. The received cumulative rewards on the best runs converge at zero, which means that the agent learns to do nothing. This is due to the rewarding function that discourages any action taken. A successful continuous Mountain Car run would receive a reward around 90. In this experiment we also do not use any hyper parameter optimization, because it fails constantly. In this case we set $\sigma_f$ and $\sigma_l$ to 1.

\section{Model parameters}

Due to the high variations in the plots, we divide the standard deviations by five before visualizing them in order to maintain the clarity of the graph. Not only the variance between trials is high, but also between iteration steps. Therefore we apply a moving mean of 15 steps on each plot too enhance readability. Maybe these high variances are an indicator for poorly selected model parameters. As shown exemplary in figure \ref{fig:noisecompare} tuning of the noise level parameter $\sigma_n$ can have a huge impact on the results. To show this impact we did some continuous Cart Pole runs with different noise level parameters.
\\
When computing the continuous action space trajectory distance metric, proposed by \cite{wilson2014using}, hyper parameter optimization is mandatory to compensate for high distance values. These values occur because of the Gaussian distribution density used as the probability measure for actions \ref{eq:contiAS}. Since we exponentiate the negative of the distance values, very high distance values will result in covariance values that are zero. The optimization of the scale length hyper parameter $\sigma_l$ will prevent all-zero covariance matrices. Experiments have shown that tuning the signal deviation parameter $\sigma_f$ is also helpful, because we simulate a policy only once for a result. Since a policy will not produce the exact same result after repeated evaluations, we have to assume a high variation on each result. This variation is regarded by $\sigma_f$. For consistency we do the hyper parameter optimization on each kernel. On test runs that improved the performance of the standard kernels as well.\\

\section{Conclusion}

The results show that trajectory kernels can have some advantage if computation time is not expensive compared to evaluation including robotic movement.
% However, the standard kernels seem to require a similar number of black box function evaluations on the experiments shown.

Unfortunately, we were not able to reproduce the outstanding results with the trajectory kernel from \ref{wilson2014using}. Maybe because they do not provide any parameters, environment settings or rewarding functions. In our experiments the overall learning performance of the trajectory kernels was either worse or slightly better than the learning performance of standard kernels. If you take the computing efficiency into consideration they perform far worse than standard kernels. Depending on the setup, the trajectory kernels took between 3 and 22 times more computational time than the squared exponential kernel (the measured times of continuous Mountain Car are not considered, because it did not solve).\\
\\
Maybe the bad learning performance of the trajectory kernel is a problem of poorly tuned hyper parameters $\sigma_f$ and $\sigma_l$, the noise level parameter $\sigma_n$, the expected improvement trade-off parameter $\tau$ or other model constraints. Apart from that, the trajectory kernel would only be worthwhile on black box functions that are quite expensively to evaluate compared to the computational effort needed for trajectory kernel calculations.
